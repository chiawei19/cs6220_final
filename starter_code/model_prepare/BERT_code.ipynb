{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BERT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ddae1fd68>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import codecs\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"./data/training/\"\n",
    "TRAIN_SUFFIX = \"-train.txt\"\n",
    "\n",
    "DEV_DIR = \"./data/development/\"\n",
    "DEV_SUFFIX = \"-dev.txt\"\n",
    "\n",
    "TEST_DIR = \"./data/test-gold/\"\n",
    "TEST_SUFFIX = \"-test-gold.txt\"\n",
    "\n",
    "YEAR_PREFIX = \"2018-\"\n",
    "FILE_PREFIX = \"EI-oc-En-\"\n",
    "\n",
    "EMOTIONS = [\"anger\", \"fear\", \"joy\", \"sadness\"] \n",
    "LABEL_EMOTIONS = {i: emo for i, emo in enumerate(EMOTIONS)}\n",
    "EMOTIONS_LABEL = {emo: i for i, emo in enumerate(EMOTIONS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/training/EI-oc-En-anger-train.txt True\n",
      "./data/training/EI-oc-En-fear-train.txt True\n",
      "./data/training/EI-oc-En-joy-train.txt True\n",
      "./data/training/EI-oc-En-sadness-train.txt True\n",
      "./data/development/2018-EI-oc-En-anger-dev.txt True\n",
      "./data/development/2018-EI-oc-En-fear-dev.txt True\n",
      "./data/development/2018-EI-oc-En-joy-dev.txt True\n",
      "./data/development/2018-EI-oc-En-sadness-dev.txt True\n",
      "./data/test-gold/2018-EI-oc-En-anger-test-gold.txt True\n",
      "./data/test-gold/2018-EI-oc-En-fear-test-gold.txt True\n",
      "./data/test-gold/2018-EI-oc-En-joy-test-gold.txt True\n",
      "./data/test-gold/2018-EI-oc-En-sadness-test-gold.txt True\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FILES = []\n",
    "DEV_FILES = []\n",
    "TEST_FILES = []\n",
    "\n",
    "for emo in EMOTIONS: \n",
    "    TRAIN_FILES += [TRAIN_DIR + FILE_PREFIX + emo + TRAIN_SUFFIX]\n",
    "    DEV_FILES += [DEV_DIR + YEAR_PREFIX + FILE_PREFIX + emo + DEV_SUFFIX]\n",
    "    TEST_FILES += [TEST_DIR + YEAR_PREFIX + FILE_PREFIX + emo + TEST_SUFFIX]\n",
    "\n",
    "for file in TRAIN_FILES + DEV_FILES + TEST_FILES:\n",
    "    print(file, os.path.exists(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2017-En-41199</td>\n",
       "      <td>Incredibly shocked and disappointed with @unit...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3: high amount of sadness can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2017-En-30873</td>\n",
       "      <td>If yiu don't respond .o an email within 7 days...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2017-En-30332</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @maisi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2: moderate amount of joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2017-En-31279</td>\n",
       "      <td>But I got to see her last when she was lively ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>2017-En-21925</td>\n",
       "      <td>@ThomasHCrown @laurakfillault As a historic ev...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0: no fear can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>2017-En-22224</td>\n",
       "      <td>today afghanistan tell us where the terrorism ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2: moderate amount of fear can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2017-En-31048</td>\n",
       "      <td>#IfIWerePresident \\nMy goal would be for us al...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>2017-En-31241</td>\n",
       "      <td>@1barkcom Thank you for the #follow. Looking f...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2: moderate amount of joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2017-En-31232</td>\n",
       "      <td>The best part of this day, Jesus, is to snuggl...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3: high amount of joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>2017-En-10088</td>\n",
       "      <td>Bes! You don't just tell a true blooded hoopju...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3: high amount of anger can be inferred</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7102 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "526   2017-En-41199  Incredibly shocked and disappointed with @unit...   \n",
       "494   2017-En-30873  If yiu don't respond .o an email within 7 days...   \n",
       "724   2017-En-30332  Watch this amazing live.ly broadcast by @maisi...   \n",
       "879   2017-En-31279  But I got to see her last when she was lively ...   \n",
       "2095  2017-En-21925  @ThomasHCrown @laurakfillault As a historic ev...   \n",
       "...             ...                                                ...   \n",
       "2071  2017-En-22224  today afghanistan tell us where the terrorism ...   \n",
       "1238  2017-En-31048  #IfIWerePresident \\nMy goal would be for us al...   \n",
       "1273  2017-En-31241  @1barkcom Thank you for the #follow. Looking f...   \n",
       "1437  2017-En-31232  The best part of this day, Jesus, is to snuggl...   \n",
       "860   2017-En-10088  Bes! You don't just tell a true blooded hoopju...   \n",
       "\n",
       "     Affect Dimension                             Intensity Class  \n",
       "526           sadness   3: high amount of sadness can be inferred  \n",
       "494               joy                   0: no joy can be inferred  \n",
       "724               joy   2: moderate amount of joy can be inferred  \n",
       "879               joy                   0: no joy can be inferred  \n",
       "2095             fear                  0: no fear can be inferred  \n",
       "...               ...                                         ...  \n",
       "2071             fear  2: moderate amount of fear can be inferred  \n",
       "1238              joy                   0: no joy can be inferred  \n",
       "1273              joy   2: moderate amount of joy can be inferred  \n",
       "1437              joy       3: high amount of joy can be inferred  \n",
       "860             anger     3: high amount of anger can be inferred  \n",
       "\n",
       "[7102 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = pd.concat(pd.read_csv(f, sep='\\t') for f in TRAIN_FILES)\n",
    "TRAIN_DATA = sklearn.utils.shuffle(TRAIN_DATA)\n",
    "TRAIN_DATA        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2017-En-41199</td>\n",
       "      <td>Incredibly shocked and disappointed with @unit...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3: high amount of sadness can be inferred</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2017-En-30873</td>\n",
       "      <td>If yiu don't respond .o an email within 7 days...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2017-En-30332</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @maisi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2: moderate amount of joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2017-En-31279</td>\n",
       "      <td>But I got to see her last when she was lively ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>2017-En-21925</td>\n",
       "      <td>@ThomasHCrown @laurakfillault As a historic ev...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0: no fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>2017-En-22224</td>\n",
       "      <td>today afghanistan tell us where the terrorism ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2: moderate amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2017-En-31048</td>\n",
       "      <td>#IfIWerePresident \\nMy goal would be for us al...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>2017-En-31241</td>\n",
       "      <td>@1barkcom Thank you for the #follow. Looking f...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2: moderate amount of joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2017-En-31232</td>\n",
       "      <td>The best part of this day, Jesus, is to snuggl...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3: high amount of joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>2017-En-10088</td>\n",
       "      <td>Bes! You don't just tell a true blooded hoopju...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3: high amount of anger can be inferred</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7102 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "526   2017-En-41199  Incredibly shocked and disappointed with @unit...   \n",
       "494   2017-En-30873  If yiu don't respond .o an email within 7 days...   \n",
       "724   2017-En-30332  Watch this amazing live.ly broadcast by @maisi...   \n",
       "879   2017-En-31279  But I got to see her last when she was lively ...   \n",
       "2095  2017-En-21925  @ThomasHCrown @laurakfillault As a historic ev...   \n",
       "...             ...                                                ...   \n",
       "2071  2017-En-22224  today afghanistan tell us where the terrorism ...   \n",
       "1238  2017-En-31048  #IfIWerePresident \\nMy goal would be for us al...   \n",
       "1273  2017-En-31241  @1barkcom Thank you for the #follow. Looking f...   \n",
       "1437  2017-En-31232  The best part of this day, Jesus, is to snuggl...   \n",
       "860   2017-En-10088  Bes! You don't just tell a true blooded hoopju...   \n",
       "\n",
       "     Affect Dimension                             Intensity Class Category  \\\n",
       "526           sadness   3: high amount of sadness can be inferred  sadness   \n",
       "494               joy                   0: no joy can be inferred      joy   \n",
       "724               joy   2: moderate amount of joy can be inferred      joy   \n",
       "879               joy                   0: no joy can be inferred      joy   \n",
       "2095             fear                  0: no fear can be inferred     fear   \n",
       "...               ...                                         ...      ...   \n",
       "2071             fear  2: moderate amount of fear can be inferred     fear   \n",
       "1238              joy                   0: no joy can be inferred      joy   \n",
       "1273              joy   2: moderate amount of joy can be inferred      joy   \n",
       "1437              joy       3: high amount of joy can be inferred      joy   \n",
       "860             anger     3: high amount of anger can be inferred    anger   \n",
       "\n",
       "      Label  Intensity  \n",
       "526       3          3  \n",
       "494       2          0  \n",
       "724       2          2  \n",
       "879       2          0  \n",
       "2095      1          0  \n",
       "...     ...        ...  \n",
       "2071      1          2  \n",
       "1238      2          0  \n",
       "1273      2          2  \n",
       "1437      2          3  \n",
       "860       0          3  \n",
       "\n",
       "[7102 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[\"Category\"] = TRAIN_DATA[\"Affect Dimension\"]\n",
    "TRAIN_DATA[\"Label\"] = TRAIN_DATA[\"Affect Dimension\"].apply(lambda x: EMOTIONS_LABEL[x])\n",
    "TRAIN_DATA[\"Intensity\"] = TRAIN_DATA[\"Intensity Class\"].apply(lambda x: int(x[0]))\n",
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fear       2252\n",
       "anger      1701\n",
       "joy        1616\n",
       "sadness    1533\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CATEGORY_WEIGHTS = TRAIN_DATA[\"Category\"].value_counts().to_dict()\n",
    "TRAIN_LABEL_WEIGHTS =  TRAIN_DATA[\"Label\"].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2018-En-01824</td>\n",
       "      <td>God this match is dull #Wimbledon</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2018-En-03227</td>\n",
       "      <td>Not only has my flight been delayed numerous t...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1: low amount of fear can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2018-En-00695</td>\n",
       "      <td>@austin_j_powers @I_am_bumface @R9Rai @Holborn...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1: low amount of fear can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2018-En-03038</td>\n",
       "      <td>James Clapper 'scary and disturbing'.  #25thAm...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2: moderate amount of fear can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2018-En-03488</td>\n",
       "      <td>I wish EVERYONE could see how #MSNBC #CNN talk...</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-En-00823</td>\n",
       "      <td>Only halfway through #madeforlove by @AlissaNu...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3: high amount of joy can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2018-En-01671</td>\n",
       "      <td>Nigga write me talking bout I dreamt about you...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0: no sadness can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2018-En-00657</td>\n",
       "      <td>Damn I'm tired as hell I never get a off day d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2: moderate amount of sadness can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2018-En-03719</td>\n",
       "      <td>The blackest abyss of despair, Alonzo</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2: moderate amount of sadness can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2018-En-00405</td>\n",
       "      <td>Media is so focused on bad news that we forget...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1: low amount of joy can be inferred</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              Tweet  \\\n",
       "321  2018-En-01824                  God this match is dull #Wimbledon   \n",
       "62   2018-En-03227  Not only has my flight been delayed numerous t...   \n",
       "327  2018-En-00695  @austin_j_powers @I_am_bumface @R9Rai @Holborn...   \n",
       "166  2018-En-03038  James Clapper 'scary and disturbing'.  #25thAm...   \n",
       "369  2018-En-03488  I wish EVERYONE could see how #MSNBC #CNN talk...   \n",
       "..             ...                                                ...   \n",
       "55   2018-En-00823  Only halfway through #madeforlove by @AlissaNu...   \n",
       "266  2018-En-01671  Nigga write me talking bout I dreamt about you...   \n",
       "207  2018-En-00657  Damn I'm tired as hell I never get a off day d...   \n",
       "235  2018-En-03719              The blackest abyss of despair, Alonzo   \n",
       "176  2018-En-00405  Media is so focused on bad news that we forget...   \n",
       "\n",
       "    Affect Dimension                                Intensity Class  \n",
       "321            anger         1: low amount of anger can be inferred  \n",
       "62              fear          1: low amount of fear can be inferred  \n",
       "327             fear          1: low amount of fear can be inferred  \n",
       "166             fear     2: moderate amount of fear can be inferred  \n",
       "369            anger         1: low amount of anger can be inferred  \n",
       "..               ...                                            ...  \n",
       "55               joy          3: high amount of joy can be inferred  \n",
       "266          sadness                  0: no sadness can be inferred  \n",
       "207          sadness  2: moderate amount of sadness can be inferred  \n",
       "235          sadness  2: moderate amount of sadness can be inferred  \n",
       "176              joy           1: low amount of joy can be inferred  \n",
       "\n",
       "[1464 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_DATA = pd.concat(pd.read_csv(f, sep='\\t') for f in DEV_FILES)\n",
    "DEV_DATA = sklearn.utils.shuffle(DEV_DATA)\n",
    "DEV_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2018-En-01824</td>\n",
       "      <td>God this match is dull #Wimbledon</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2018-En-03227</td>\n",
       "      <td>Not only has my flight been delayed numerous t...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1: low amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2018-En-00695</td>\n",
       "      <td>@austin_j_powers @I_am_bumface @R9Rai @Holborn...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1: low amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2018-En-03038</td>\n",
       "      <td>James Clapper 'scary and disturbing'.  #25thAm...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2: moderate amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2018-En-03488</td>\n",
       "      <td>I wish EVERYONE could see how #MSNBC #CNN talk...</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-En-00823</td>\n",
       "      <td>Only halfway through #madeforlove by @AlissaNu...</td>\n",
       "      <td>joy</td>\n",
       "      <td>3: high amount of joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2018-En-01671</td>\n",
       "      <td>Nigga write me talking bout I dreamt about you...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0: no sadness can be inferred</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2018-En-00657</td>\n",
       "      <td>Damn I'm tired as hell I never get a off day d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2: moderate amount of sadness can be inferred</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2018-En-03719</td>\n",
       "      <td>The blackest abyss of despair, Alonzo</td>\n",
       "      <td>sadness</td>\n",
       "      <td>2: moderate amount of sadness can be inferred</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2018-En-00405</td>\n",
       "      <td>Media is so focused on bad news that we forget...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1: low amount of joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1464 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              Tweet  \\\n",
       "321  2018-En-01824                  God this match is dull #Wimbledon   \n",
       "62   2018-En-03227  Not only has my flight been delayed numerous t...   \n",
       "327  2018-En-00695  @austin_j_powers @I_am_bumface @R9Rai @Holborn...   \n",
       "166  2018-En-03038  James Clapper 'scary and disturbing'.  #25thAm...   \n",
       "369  2018-En-03488  I wish EVERYONE could see how #MSNBC #CNN talk...   \n",
       "..             ...                                                ...   \n",
       "55   2018-En-00823  Only halfway through #madeforlove by @AlissaNu...   \n",
       "266  2018-En-01671  Nigga write me talking bout I dreamt about you...   \n",
       "207  2018-En-00657  Damn I'm tired as hell I never get a off day d...   \n",
       "235  2018-En-03719              The blackest abyss of despair, Alonzo   \n",
       "176  2018-En-00405  Media is so focused on bad news that we forget...   \n",
       "\n",
       "    Affect Dimension                                Intensity Class Category  \\\n",
       "321            anger         1: low amount of anger can be inferred    anger   \n",
       "62              fear          1: low amount of fear can be inferred     fear   \n",
       "327             fear          1: low amount of fear can be inferred     fear   \n",
       "166             fear     2: moderate amount of fear can be inferred     fear   \n",
       "369            anger         1: low amount of anger can be inferred    anger   \n",
       "..               ...                                            ...      ...   \n",
       "55               joy          3: high amount of joy can be inferred      joy   \n",
       "266          sadness                  0: no sadness can be inferred  sadness   \n",
       "207          sadness  2: moderate amount of sadness can be inferred  sadness   \n",
       "235          sadness  2: moderate amount of sadness can be inferred  sadness   \n",
       "176              joy           1: low amount of joy can be inferred      joy   \n",
       "\n",
       "     Label  Intensity  \n",
       "321      0          1  \n",
       "62       1          1  \n",
       "327      1          1  \n",
       "166      1          2  \n",
       "369      0          1  \n",
       "..     ...        ...  \n",
       "55       2          3  \n",
       "266      3          0  \n",
       "207      3          2  \n",
       "235      3          2  \n",
       "176      2          1  \n",
       "\n",
       "[1464 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_DATA[\"Category\"] = DEV_DATA[\"Affect Dimension\"]\n",
    "DEV_DATA[\"Label\"] = DEV_DATA[\"Affect Dimension\"].apply(lambda x: EMOTIONS_LABEL[x])\n",
    "DEV_DATA[\"Intensity\"] = DEV_DATA[\"Intensity Class\"].apply(lambda x: int(x[0]))\n",
    "DEV_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cwh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#For Pre-Processing\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# For visualizing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_emoji(text):\n",
    "    output = []\n",
    "    for t in emoji.demojize(text).split():\n",
    "        output.extend(t.split(\"_\"))\n",
    "    return ' '.join(output)\n",
    "\n",
    "def decode_HTML(text):\n",
    "    return BeautifulSoup(text, 'lxml').get_text()\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    \n",
    "def remove_URL(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def correct_repeatted_letters(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return ' '.join([w.lower() for w in text.split()])\n",
    " \n",
    "def lemmatize_stemming(text):\n",
    "    return ' '.join([stemmer.stem(lemmatizer.lemmatize(w, pos='v'))\n",
    "                     for w in text.split()])\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([w for w in text.split() if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    text = decode_emoji(text)\n",
    "    text = decode_HTML(text)\n",
    "    text = remove_mention(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = correct_repeatted_letters(text)\n",
    "    text = to_lowercase(text)       \n",
    "    text = lemmatize_stemming(text)\n",
    "    text = remove_stop_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: Incredibly shocked and disappointed with @united customer service. Really making me rethink flying with them in the future. #unhappy\n",
      "AFTER : incred shock disappoint custom servic realli make rethink fli futur unhappi\n",
      "\n",
      "BEFORE: If yiu don't respond .o an email within 7 days, you willxbe killed by an animated gif of the girl froa The Ri.g.\n",
      "AFTER : yiu respond email within 7 day willxb kill anim gif girl froa ri g\n",
      "\n",
      "BEFORE: Watch this amazing live.ly broadcast by @maisiev #lively #musically\n",
      "AFTER : watch amaz live ly broadcast live music\n",
      "\n",
      "BEFORE: But I got to see her last when she was lively and talkative and I was able to tell her I loved her so that's what matters.\n",
      "AFTER : get see last live talkat abl tell love matter\n",
      "\n",
      "BEFORE: @ThomasHCrown @laurakfillault As a historic evangelical, I wonder if the Donatists had a point? What dreadful clergy in my fmr. circles.\n",
      "AFTER : histor evangel wonder donatist point dread clergi fmr circl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in TRAIN_DATA[\"Tweet\"][:5]:\n",
    "    print(\"BEFORE:\", t)\n",
    "    print(\"AFTER :\", pre_process(t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2017-En-41199</td>\n",
       "      <td>Incredibly shocked and disappointed with @unit...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3: high amount of sadness can be inferred</td>\n",
       "      <td>sadness</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>incred shock disappoint custom servic realli m...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2017-En-30873</td>\n",
       "      <td>If yiu don't respond .o an email within 7 days...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yiu respond email within 7 day willxb kill ani...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2017-En-30332</td>\n",
       "      <td>Watch this amazing live.ly broadcast by @maisi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2: moderate amount of joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>watch amaz live ly broadcast live music</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2017-En-31279</td>\n",
       "      <td>But I got to see her last when she was lively ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0: no joy can be inferred</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>get see last live talkat abl tell love matter</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>2017-En-21925</td>\n",
       "      <td>@ThomasHCrown @laurakfillault As a historic ev...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0: no fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>histor evangel wonder donatist point dread cle...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "526   2017-En-41199  Incredibly shocked and disappointed with @unit...   \n",
       "494   2017-En-30873  If yiu don't respond .o an email within 7 days...   \n",
       "724   2017-En-30332  Watch this amazing live.ly broadcast by @maisi...   \n",
       "879   2017-En-31279  But I got to see her last when she was lively ...   \n",
       "2095  2017-En-21925  @ThomasHCrown @laurakfillault As a historic ev...   \n",
       "\n",
       "     Affect Dimension                            Intensity Class Category  \\\n",
       "526           sadness  3: high amount of sadness can be inferred  sadness   \n",
       "494               joy                  0: no joy can be inferred      joy   \n",
       "724               joy  2: moderate amount of joy can be inferred      joy   \n",
       "879               joy                  0: no joy can be inferred      joy   \n",
       "2095             fear                 0: no fear can be inferred     fear   \n",
       "\n",
       "      Label  Intensity                                               Text  \\\n",
       "526       3          3  incred shock disappoint custom servic realli m...   \n",
       "494       2          0  yiu respond email within 7 day willxb kill ani...   \n",
       "724       2          2            watch amaz live ly broadcast live music   \n",
       "879       2          0      get see last live talkat abl tell love matter   \n",
       "2095      1          0  histor evangel wonder donatist point dread cle...   \n",
       "\n",
       "      Text_len  \n",
       "526         11  \n",
       "494         14  \n",
       "724          7  \n",
       "879          9  \n",
       "2095         9  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data cleaning\n",
    "TRAIN_DATA[\"Text\"] = TRAIN_DATA[\"Tweet\"].apply(lambda s: pre_process(s))\n",
    "TRAIN_DATA[\"Text_len\"] = TRAIN_DATA[\"Text\"].apply(lambda s: len(s.split()))\n",
    "TRAIN_DATA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2018-En-01824</td>\n",
       "      <td>God this match is dull #Wimbledon</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>god match dull wimbledon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2018-En-03227</td>\n",
       "      <td>Not only has my flight been delayed numerous t...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1: low amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>onli flight delay numer time provid snack cart...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2018-En-00695</td>\n",
       "      <td>@austin_j_powers @I_am_bumface @R9Rai @Holborn...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1: low amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>j power bumfac absolut shock behavour</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2018-En-03038</td>\n",
       "      <td>James Clapper 'scary and disturbing'.  #25thAm...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2: moderate amount of fear can be inferred</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>jam clapper scari disturb 25thamendmentnow imp...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2018-En-03488</td>\n",
       "      <td>I wish EVERYONE could see how #MSNBC #CNN talk...</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>wish everyon could see msnbc cnn talk trump sp...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              Tweet  \\\n",
       "321  2018-En-01824                  God this match is dull #Wimbledon   \n",
       "62   2018-En-03227  Not only has my flight been delayed numerous t...   \n",
       "327  2018-En-00695  @austin_j_powers @I_am_bumface @R9Rai @Holborn...   \n",
       "166  2018-En-03038  James Clapper 'scary and disturbing'.  #25thAm...   \n",
       "369  2018-En-03488  I wish EVERYONE could see how #MSNBC #CNN talk...   \n",
       "\n",
       "    Affect Dimension                             Intensity Class Category  \\\n",
       "321            anger      1: low amount of anger can be inferred    anger   \n",
       "62              fear       1: low amount of fear can be inferred     fear   \n",
       "327             fear       1: low amount of fear can be inferred     fear   \n",
       "166             fear  2: moderate amount of fear can be inferred     fear   \n",
       "369            anger      1: low amount of anger can be inferred    anger   \n",
       "\n",
       "     Label  Intensity                                               Text  \\\n",
       "321      0          1                           god match dull wimbledon   \n",
       "62       1          1  onli flight delay numer time provid snack cart...   \n",
       "327      1          1              j power bumfac absolut shock behavour   \n",
       "166      1          2  jam clapper scari disturb 25thamendmentnow imp...   \n",
       "369      0          1  wish everyon could see msnbc cnn talk trump sp...   \n",
       "\n",
       "     Text_len  \n",
       "321         4  \n",
       "62          9  \n",
       "327         6  \n",
       "166         9  \n",
       "369        13  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dev data cleaning\n",
    "DEV_DATA[\"Text\"] = DEV_DATA[\"Tweet\"].apply(lambda s: pre_process(s))\n",
    "DEV_DATA[\"Text_len\"] = DEV_DATA[\"Text\"].apply(lambda s: len(s.split()))\n",
    "DEV_DATA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    7102.000000\n",
      "mean        9.513377\n",
      "std         4.402350\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%         9.000000\n",
      "75%        12.000000\n",
      "max        79.000000\n",
      "Name: Text_len, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(TRAIN_DATA[\"Text_len\"].describe())\n",
    "\n",
    "plt.hist(TRAIN_DATA[\"Text_len\"], color='blue', edgecolor='black')\n",
    "plt.title('Histogram of cleaned tweet length')\n",
    "plt.xlabel('length (word)')\n",
    "plt.ylabel('tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: i will never watch greys anatomy ever ever ever ever ever again if Shonda Rimes takes away another OG character☹️☹️☹️☹️☹️☹️☹️☹️☹️☹️ #fuming\n",
      "AFTER : never watch grey anatomi ever ever ever ever ever shonda rim take away anoth og charact frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector fume\n",
      "\n",
      "BEFORE: Good morning, Trondheim! #optimism #productivity ⛅️❤️🇳🇴🏢💻🖥🏋🏻💪🏼📺🍿\n",
      "AFTER : good morn trondheim optim product sun behind cloud red heart selector norway offic build laptop comput desktop comput person lift weight light skin tone flex bicep medium light skin tone televis popcorn\n",
      "\n",
      "BEFORE: i will never watch greys anatomy ever ever ever ever ever again if Shonda Rimes takes away another OG character☹️☹️☹️☹️☹️☹️☹️☹️☹️☹️ \n",
      "AFTER : never watch grey anatomi ever ever ever ever ever shonda rim take away anoth og charact frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector frown face selector\n",
      "\n",
      "BEFORE: @priny_baby happppy happppyyyyyy happppppyyyyy haaapppyyyy birthday best friend!! Love you lots 💖💖💖💖💖💖💖🎉🎊 #chapter22 #bdaygirl #happy #love\n",
      "AFTER : babi happi happyy happyy haappyy birthday best friend love lot sparkl heart sparkl heart sparkl heart sparkl heart sparkl heart sparkl heart sparkl heart parti popper confetti ball chapter22 bdaygirl happi love\n",
      "\n",
      "BEFORE: semores: let's get this. #mirth #SAW #keepthetradition 💜💛⚛🏵🍇🍋🌼🐤🐥👚😈😈👾🧀\n",
      "AFTER : semor let get mirth saw keepthetradit purpl heart yellow heart atom symbol rosett grape lemon blossom babi chick front face babi chick woman cloth smile face horn smile face horn alien monster chees wedg\n",
      "\n",
      "BEFORE: Nahhhhhh @konanplaydirty snap story has got me bussing up 😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "AFTER : nahh snap stori get buss face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy face tear joy\n",
      "\n",
      "BEFORE: Think I really broke young bull hurt 😂😂😭😭 u can tell by how bitter and mad he is 😊😊😩😩\n",
      "AFTER : think realli break young bull hurt face tear joy face tear joy loud cri face loud cri face u tell bitter mad smile face smile eye smile face smile eye weari face weari face\n",
      "\n",
      "BEFORE: Trying to think positive, and not let this situation discourage me ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\n",
      "AFTER : tri think posit let situat discourag sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl sparkl\n",
      "\n",
      "BEFORE: @priny_baby happppy happppyyyyyy happppppyyyyy haaapppyyyy birthday best friend!! Love you lots 💖💖💖💖💖💖💖🎉🎊 #chapter22 #bdaygirl  #love\n",
      "AFTER : babi happi happyy happyy haappyy birthday best friend love lot sparkl heart sparkl heart sparkl heart sparkl heart sparkl heart sparkl heart sparkl heart parti popper confetti ball chapter22 bdaygirl love\n",
      "\n",
      "BEFORE: Fast and furious 6 this Monday 10 pm on mbc 2 😍😍😍😍😍😍😍\n",
      "AFTER : fast furious 6 monday 10 pm mbc 2 smile face heart eye smile face heart eye smile face heart eye smile face heart eye smile face heart eye smile face heart eye smile face heart eye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, t in TRAIN_DATA.iterrows():\n",
    "    if t[\"Text_len\"] > 30:\n",
    "        print(\"BEFORE:\", t[\"Tweet\"])\n",
    "        print(\"AFTER :\", t[\"Text\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(data):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for X, y in zip(data[\"Tweet\"], data[\"Label\"]):\n",
    "        words = pre_process(X).split()\n",
    "        if len(words) > 0:\n",
    "            X_data.append(pre_process(X))\n",
    "            y_data.append(y)\n",
    "            \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prep(TRAIN_DATA)\n",
    "X_dev, y_dev = prep(DEV_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['incred shock disappoint custom servic realli make rethink fli futur unhappi',\n",
       "  'yiu respond email within 7 day willxb kill anim gif girl froa ri g',\n",
       "  'watch amaz live ly broadcast live music',\n",
       "  'get see last live talkat abl tell love matter',\n",
       "  'histor evangel wonder donatist point dread clergi fmr circl',\n",
       "  'let know',\n",
       "  'fuck dread live footi match',\n",
       "  'fell hear snap hffhj',\n",
       "  'imagin stupid tri chirp becaus racist prick',\n",
       "  'least crystal everi part portray wrong dark atmospher fact mamoru brainwash'],\n",
       " [3, 2, 2, 2, 1, 3, 1, 0, 2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = prep(TRAIN_DATA[:10])\n",
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23950999718389185,\n",
       " 0.31709377640101377,\n",
       " 0.22754153759504364,\n",
       " 0.21585468882005068]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution = collections.Counter(y_train)\n",
    "class_weights = [label_distribution[i] / sum(label_distribution.values()) for i in range(4)]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './BERT_working/saved_weights.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, 1)\n",
    "    return torch.sum(pred == label).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the accuracy and loss\n",
    "def plot_acc_loss(tr_acc, vl_acc, tr_loss, vl_loss):\n",
    "    fig = plt.figure(figsize = (20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "    plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_tr_loss, label='Train loss')\n",
    "    plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.normal_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    tokens_text = tokenizer.batch_encode_plus(\n",
    "        text,\n",
    "        max_length=25,\n",
    "        padding=True,\n",
    "#         pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    seq = torch.tensor(tokens_text['input_ids'])\n",
    "    mask = torch.tensor(tokens_text['attention_mask'])\n",
    "    return seq, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds= tensor([1, 2, 1, 1, 2, 1, 2, 2, 2, 2])\n",
      "labels= tensor([3, 2, 2, 2, 1, 3, 1, 0, 2, 3])\n",
      "acc= 2\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BERT_1(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):      \n",
    "        super(BERT_1, self).__init__()\n",
    "        self.bert = bert\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "                 \n",
    "        self.output = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        out, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        x = F.relu(cls_hs)\n",
    "        x = self.output(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "m = BERT_1(bert)\n",
    "m.apply(weights_init_uniform)\n",
    "inputs, masks = tokenizing(X_test)\n",
    "labels = torch.Tensor(y_test).long()\n",
    "preds = m(inputs, masks)\n",
    "print(\"preds=\",torch.argmax(preds, 1))\n",
    "print(\"labels=\", labels)\n",
    "print(\"acc=\", acc(preds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds= tensor([3, 0, 3, 0, 0, 3, 0, 1, 0, 0])\n",
      "labels= tensor([3, 2, 2, 2, 1, 3, 1, 0, 2, 3])\n",
      "acc= 2\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BERT_2(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):      \n",
    "        super(BERT_2, self).__init__()\n",
    "        self.bert = bert\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False          \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768, 128)       \n",
    "        self.output = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        out, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "       \n",
    "        x = self.fc1(F.relu(cls_hs))\n",
    "        x = self.dropout1(x)\n",
    "        x = cls_hs\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "m = BERT_2(bert)\n",
    "# m.apply(weights_init_uniform)\n",
    "inputs, masks = tokenizing(X_test)\n",
    "labels = torch.Tensor(y_test).long()\n",
    "preds = m(inputs, masks)\n",
    "print(\"preds=\",torch.argmax(preds, 1))\n",
    "print(\"labels=\", labels)\n",
    "print(\"acc=\", acc(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds=[]\n",
    "        \n",
    "    for start in tqdm(range(0, len(X_train), batch_size)):\n",
    "\n",
    "        inputs, masks = tokenizing(X_train[start: start+batch_size])\n",
    "        labels = torch.Tensor(y_train[start: start+batch_size]).long()\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        preds = model(inputs, masks)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "        total_preds.extend(preds)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "    avg_loss = total_loss / len(X_train) \n",
    "    total_preds = [torch.argmax(p, 0) for p in total_preds]\n",
    "    \n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = []\n",
    "\n",
    "    for start in range(0, len(X_dev), batch_size):\n",
    "\n",
    "        inputs, masks = tokenizing(X_dev[start: start+batch_size])\n",
    "        labels = torch.Tensor(y_dev[start: start+batch_size]).long()\n",
    "        \n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = model(inputs, masks)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            total_preds.extend(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(X_dev) \n",
    "    total_preds = [int(torch.argmax(p, 0)) for p in total_preds]\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "model = BERT_1(bert)\n",
    "model.apply(weights_init_uniform)\n",
    "\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert For Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:09<00:00, 13.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.090638\n",
      "Validation Loss: 0.073022\n",
      "Validation Acc: 0.2652084757347915\n",
      "\n",
      " Epoch 2 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [11:49<00:00, 12.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.079705\n",
      "Validation Loss: 0.067483\n",
      "Validation Acc: 0.2693096377306904\n",
      "\n",
      " Epoch 3 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [11:40<00:00, 12.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.072618\n",
      "Validation Loss: 0.061018\n",
      "Validation Acc: 0.2638414217361586\n",
      "\n",
      " Epoch 4 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [11:35<00:00, 12.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.065748\n",
      "Validation Loss: 0.053444\n",
      "Validation Acc: 0.2563226247436774\n",
      "\n",
      " Epoch 5 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:10<00:00, 13.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.058255\n",
      "Validation Loss: 0.050751\n",
      "Validation Acc: 0.252904989747095\n",
      "\n",
      " Epoch 6 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [11:58<00:00, 12.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.054437\n",
      "Validation Loss: 0.045745\n",
      "Validation Acc: 0.24606971975393027\n",
      "\n",
      " Epoch 7 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:06<00:00, 12.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.049665\n",
      "Validation Loss: 0.044354\n",
      "Validation Acc: 0.2535885167464115\n",
      "\n",
      " Epoch 8 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:03<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.044915\n",
      "Validation Loss: 0.040259\n",
      "Validation Acc: 0.2576896787423103\n",
      "\n",
      " Epoch 9 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [11:49<00:00, 12.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.042340\n",
      "Validation Loss: 0.034203\n",
      "Validation Acc: 0.2522214627477785\n",
      "\n",
      " Epoch 10 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:17<00:00, 13.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.038215\n",
      "Validation Loss: 0.036425\n",
      "Validation Acc: 0.2583732057416268\n",
      "\n",
      " Epoch 11 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:26<00:00, 13.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.034931\n",
      "Validation Loss: 0.029965\n",
      "Validation Acc: 0.2522214627477785\n",
      "\n",
      " Epoch 12 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:24<00:00, 13.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.032816\n",
      "Validation Loss: 0.032441\n",
      "Validation Acc: 0.2583732057416268\n",
      "\n",
      " Epoch 13 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:23<00:00, 13.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.030237\n",
      "Validation Loss: 0.022817\n",
      "Validation Acc: 0.2508544087491456\n",
      "\n",
      " Epoch 14 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:27<00:00, 13.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.028037\n",
      "Validation Loss: 0.021570\n",
      "Validation Acc: 0.2576896787423103\n",
      "\n",
      " Epoch 15 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:26<00:00, 13.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.025989\n",
      "Validation Loss: 0.019739\n",
      "Validation Acc: 0.2522214627477785\n",
      "\n",
      " Epoch 16 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:24<00:00, 13.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.023389\n",
      "Validation Loss: 0.019000\n",
      "Validation Acc: 0.2535885167464115\n",
      "\n",
      " Epoch 17 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:21<00:00, 13.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.023096\n",
      "Validation Loss: 0.019545\n",
      "Validation Acc: 0.2611073137388927\n",
      "\n",
      " Epoch 18 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:34<00:00, 13.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.020951\n",
      "Validation Loss: 0.018232\n",
      "Validation Acc: 0.2631578947368421\n",
      "\n",
      " Epoch 19 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:48<00:00, 13.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.020610\n",
      "Validation Loss: 0.018655\n",
      "Validation Acc: 0.2672590567327409\n",
      "\n",
      " Epoch 20 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:45<00:00, 13.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.018642\n",
      "Validation Loss: 0.015576\n",
      "Validation Acc: 0.25153793574846206\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch+1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, y_pred = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.6f}')\n",
    "    print(f'Validation Loss: {valid_loss:.6f}')\n",
    "    print(f'Validation Acc: {accuracy_score(y_dev, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:50<00:00, 13.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.086547\n",
      "Validation Loss: 0.067970\n",
      "Validation Acc: 0.2631578947368421\n",
      "\n",
      " Epoch 2 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:46<00:00, 13.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.076490\n",
      "Validation Loss: 0.068309\n",
      "Validation Acc: 0.2679425837320574\n",
      "\n",
      " Epoch 3 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:54<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.068645\n",
      "Validation Loss: 0.059060\n",
      "Validation Acc: 0.26452494873547505\n",
      "\n",
      " Epoch 4 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:54<00:00, 13.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.062321\n",
      "Validation Loss: 0.054547\n",
      "Validation Acc: 0.26452494873547505\n",
      "\n",
      " Epoch 5 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:51<00:00, 13.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.055710\n",
      "Validation Loss: 0.049546\n",
      "Validation Acc: 0.27204374572795625\n",
      "\n",
      " Epoch 6 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:47<00:00, 13.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.050315\n",
      "Validation Loss: 0.038844\n",
      "Validation Acc: 0.2583732057416268\n",
      "\n",
      " Epoch 7 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:53<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.047289\n",
      "Validation Loss: 0.038445\n",
      "Validation Acc: 0.2652084757347915\n",
      "\n",
      " Epoch 8 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:51<00:00, 13.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.042049\n",
      "Validation Loss: 0.031958\n",
      "Validation Acc: 0.2638414217361586\n",
      "\n",
      " Epoch 9 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:47<00:00, 13.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.037897\n",
      "Validation Loss: 0.028991\n",
      "Validation Acc: 0.2604237867395762\n",
      "\n",
      " Epoch 10 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:50<00:00, 13.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.034754\n",
      "Validation Loss: 0.027737\n",
      "Validation Acc: 0.254272043745728\n",
      "\n",
      " Epoch 11 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:51<00:00, 13.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.032368\n",
      "Validation Loss: 0.028621\n",
      "Validation Acc: 0.26999316473000684\n",
      "\n",
      " Epoch 12 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:45<00:00, 13.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.030145\n",
      "Validation Loss: 0.026748\n",
      "Validation Acc: 0.2652084757347915\n",
      "\n",
      " Epoch 13 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:54<00:00, 13.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.027865\n",
      "Validation Loss: 0.026831\n",
      "Validation Acc: 0.2693096377306904\n",
      "\n",
      " Epoch 14 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [12:39<00:00, 13.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.025390\n",
      "Validation Loss: 0.024761\n",
      "Validation Acc: 0.26247436773752564\n",
      "\n",
      " Epoch 15 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [10:06<00:00, 10.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.023612\n",
      "Validation Loss: 0.025774\n",
      "Validation Acc: 0.2679425837320574\n",
      "\n",
      " Epoch 16 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [08:35<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.021724\n",
      "Validation Loss: 0.021767\n",
      "Validation Acc: 0.2672590567327409\n",
      "\n",
      " Epoch 17 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [08:39<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.020218\n",
      "Validation Loss: 0.022892\n",
      "Validation Acc: 0.26999316473000684\n",
      "\n",
      " Epoch 18 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [10:16<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.019518\n",
      "Validation Loss: 0.018379\n",
      "Validation Acc: 0.26657552973342447\n",
      "\n",
      " Epoch 19 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [09:02<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 0.018290\n",
      "Validation Loss: 0.016805\n",
      "Validation Acc: 0.26657552973342447\n",
      "\n",
      " Epoch 20 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [11:25<00:00, 12.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.017307\n",
      "Validation Loss: 0.015800\n",
      "Validation Acc: 0.2604237867395762\n"
     ]
    }
   ],
   "source": [
    "model = BERT_2(bert)\n",
    "model.apply(weights_init_uniform)\n",
    "\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch+1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, y_pred = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.6f}')\n",
    "    print(f'Validation Loss: {valid_loss:.6f}')\n",
    "    print(f'Validation Acc: {accuracy_score(y_dev, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 4, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "?? <class 'torch.Tensor'> tensor([[ 0.8951, -0.6133,  0.1833,  0.4108],\n",
      "        [ 0.9940, -0.5478,  0.5503, -0.1642],\n",
      "        [ 0.9194, -0.4019,  0.3975, -0.0298],\n",
      "        [ 0.9032, -0.3978,  0.5459,  0.0700],\n",
      "        [ 1.0448, -0.7026,  0.4215,  0.0261],\n",
      "        [ 0.9989, -0.6710,  0.3164,  0.1071],\n",
      "        [ 0.9330, -0.4710,  0.1595,  0.3788],\n",
      "        [ 0.9166, -0.4730,  0.6696, -0.2161],\n",
      "        [ 1.1538, -0.3950,  0.6913,  0.0538],\n",
      "        [ 0.9665, -0.2964,  0.2146,  0.1344],\n",
      "        [ 1.0562, -0.4522,  0.4149,  0.0110],\n",
      "        [ 0.9389, -0.4717,  0.1992,  0.1669],\n",
      "        [ 0.9330, -0.4296,  0.3340,  0.2592],\n",
      "        [ 0.9921, -0.5938,  0.5686,  0.0043],\n",
      "        [ 1.0906, -0.6172,  0.5051,  0.0383],\n",
      "        [ 0.8713, -0.4409,  0.2331,  0.3689],\n",
      "        [ 1.1841, -0.5092,  0.2990,  0.0965],\n",
      "        [ 1.0691, -0.4632,  0.5147, -0.1548],\n",
      "        [ 1.0943, -0.5166,  0.4472, -0.0991],\n",
      "        [ 0.9800, -0.6331,  0.4770,  0.1155],\n",
      "        [ 1.0224, -0.6312,  0.4810,  0.1104],\n",
      "        [ 0.9522, -0.7810,  0.2405,  0.2469],\n",
      "        [ 0.6317, -0.5994,  0.2886,  0.3077],\n",
      "        [ 1.1223, -0.4439,  0.6079,  0.0950],\n",
      "        [ 1.0060, -0.7289,  0.1631,  0.3715],\n",
      "        [ 1.0027, -0.5880,  0.7552, -0.0820],\n",
      "        [ 0.9660, -0.5907,  0.4217, -0.1877],\n",
      "        [ 1.1197, -0.5083,  0.1094,  0.3522],\n",
      "        [ 1.2559, -0.7534,  0.6987, -0.0947],\n",
      "        [ 0.6981, -0.1071,  0.2268,  0.1809],\n",
      "        [ 0.9138, -0.4754,  0.5041, -0.1624],\n",
      "        [ 0.9117, -0.3898,  0.5766, -0.0657],\n",
      "        [ 0.8102, -0.4885,  0.2691,  0.3565],\n",
      "        [ 1.0725, -0.6243,  0.4579, -0.0393],\n",
      "        [ 0.9717, -0.5763,  0.6118, -0.0504],\n",
      "        [ 0.9912, -0.8714,  0.5007, -0.0384],\n",
      "        [ 0.9147, -0.4126,  0.1677,  0.3824],\n",
      "        [ 0.9507, -0.4294,  0.2980, -0.1103],\n",
      "        [ 1.2784, -0.8232,  0.8283,  0.0216],\n",
      "        [ 1.0187, -0.4911,  0.1111,  0.0865],\n",
      "        [ 1.1782, -0.6712,  0.6478, -0.0839],\n",
      "        [ 1.1422, -0.3603,  0.5122, -0.1567],\n",
      "        [ 0.8966, -0.5072,  0.3099,  0.2507],\n",
      "        [ 1.2018, -0.5994,  0.7226, -0.2040],\n",
      "        [ 0.9383, -0.3494,  0.5193,  0.2339],\n",
      "        [ 0.9447, -0.4484,  0.2682,  0.1471],\n",
      "        [ 0.8936, -0.7293,  0.3098,  0.2032],\n",
      "        [ 0.6265, -0.4951,  0.1083,  0.1901],\n",
      "        [ 0.9251, -0.6217,  0.3417,  0.3042],\n",
      "        [ 0.9940, -0.4582,  0.1222,  0.2122],\n",
      "        [ 1.0298, -0.6411,  0.5176,  0.3454],\n",
      "        [ 1.1709, -0.5745,  0.7785,  0.1281],\n",
      "        [ 1.0830, -0.6047,  0.3786,  0.0223],\n",
      "        [ 1.1444, -0.3855,  0.6657,  0.1206],\n",
      "        [ 1.0438, -0.4854,  0.4088, -0.1587],\n",
      "        [ 0.8661, -0.4372,  0.0726,  0.2403],\n",
      "        [ 1.0430, -0.7675,  0.3383,  0.2905],\n",
      "        [ 1.0516, -0.4708,  0.5718, -0.3383],\n",
      "        [ 1.0505, -0.4002,  0.5026,  0.0156],\n",
      "        [ 1.2838, -0.3512,  0.6560,  0.1273],\n",
      "        [ 1.2192, -0.5322,  0.3421,  0.0061],\n",
      "        [ 1.0278, -0.5711,  0.4970,  0.1821],\n",
      "        [ 1.2204, -0.6751,  0.6642,  0.0187],\n",
      "        [ 0.9141, -0.6102,  0.5976, -0.0346],\n",
      "        [ 1.0542, -0.6494,  0.4863,  0.0675],\n",
      "        [ 1.0641, -0.5329,  0.3617,  0.0335],\n",
      "        [ 1.3349, -0.4753,  0.4342,  0.1248],\n",
      "        [ 1.1236, -0.5839,  0.5830, -0.0448],\n",
      "        [ 1.0682, -0.2943,  0.5207,  0.2489],\n",
      "        [ 1.3823, -0.5643,  0.6248, -0.0576],\n",
      "        [ 0.9393, -0.3978,  0.5142,  0.0586],\n",
      "        [ 1.2193, -0.3599,  0.6265, -0.1598],\n",
      "        [ 1.1015, -0.6198,  0.5730,  0.0322],\n",
      "        [ 1.1803, -0.6723,  0.3296,  0.1620],\n",
      "        [ 1.0331, -0.3846,  0.4828,  0.1017],\n",
      "        [ 0.8759, -0.6336,  0.4924,  0.2599],\n",
      "        [ 1.1933, -0.4849,  0.6495, -0.0941],\n",
      "        [ 1.0597, -0.4765,  0.1847,  0.2768],\n",
      "        [ 0.9939, -0.3948,  0.3289,  0.2051],\n",
      "        [ 1.0904, -0.5158,  0.4446,  0.1292],\n",
      "        [ 1.1165, -0.5810,  0.3253, -0.0934],\n",
      "        [ 0.8193, -0.3869,  0.2204, -0.0050],\n",
      "        [ 1.2219, -0.5851,  0.4889, -0.0158],\n",
      "        [ 1.1295, -0.6783,  0.0905,  0.1494],\n",
      "        [ 0.9625, -0.3204,  0.3642,  0.1407],\n",
      "        [ 1.0485, -0.4397,  0.0499,  0.0355],\n",
      "        [ 0.9802, -0.6597,  0.3485,  0.0877],\n",
      "        [ 1.1613, -0.5816,  0.6087, -0.1264],\n",
      "        [ 0.8747, -0.5201,  0.2578,  0.1654],\n",
      "        [ 1.2758, -0.7557,  0.4226, -0.1279],\n",
      "        [ 0.9118, -0.5579,  0.3909,  0.0802],\n",
      "        [ 1.2054, -0.5099,  0.6909,  0.0055],\n",
      "        [ 1.1634, -0.4342,  0.4302,  0.2023],\n",
      "        [ 0.9895, -0.4597,  0.4932,  0.1223],\n",
      "        [ 0.7182, -0.5345,  0.2021,  0.4301],\n",
      "        [ 0.8419, -0.4467,  0.0517,  0.3672],\n",
      "        [ 1.3135, -0.6307,  0.6445,  0.1488],\n",
      "        [ 1.0246, -0.4206,  0.4129, -0.0507],\n",
      "        [ 1.2529, -0.7685,  0.3883, -0.1745],\n",
      "        [ 1.1277, -0.3133,  0.7189, -0.2342]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d20f1f3dd69f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-ddbe18a4be1b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 4, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
